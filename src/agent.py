"""Video Fraud Detection Agent.

A security expert agent specialized in investigating video frauds
and detecting AI-generated video content.
"""

import base64
import json
import os
import tempfile
from dataclasses import dataclass
from enum import Enum
from pathlib import Path

import cv2
from dotenv import load_dotenv

load_dotenv()


class Verdict(Enum):
    """Classification verdict for video analysis."""

    AI_GENERATED = "ai_generated"
    AUTHENTIC = "authentic"
    UNCERTAIN = "uncertain"


@dataclass
class AnalysisResult:
    """Result of video fraud analysis."""

    verdict: Verdict
    confidence: float  # 0.0 to 1.0
    reasoning: str
    indicators: list[str]
    recommendations: list[str]


class VideoFraudDetectionAgent:
    """Security expert agent for detecting AI-generated videos.

    This agent analyzes video content to determine if it was
    generated by AI or is authentic footage.
    """

    SYSTEM_PROMPT = """You are a senior security expert specializing in digital forensics
and video fraud investigation. Your expertise includes:

- Detecting AI-generated video content (deepfakes, synthetic media)
- Analyzing visual artifacts and inconsistencies
- Identifying manipulation patterns in video frames
- Assessing authenticity of digital media

When analyzing a video or video frame, you should look for:

1. FACIAL ANALYSIS:
   - Unnatural facial movements or expressions
   - Inconsistent lighting on face vs background
   - Blurring around face edges or hair
   - Eye blinking patterns (too regular or absent)
   - Teeth and mouth rendering issues

2. TEMPORAL CONSISTENCY:
   - Flickering or morphing between frames
   - Inconsistent shadows across time
   - Unnatural motion blur
   - Audio-visual sync issues

3. TECHNICAL ARTIFACTS:
   - Compression artifacts in unusual places
   - Resolution inconsistencies
   - Color banding or unusual gradients
   - Edge artifacts around subjects

4. CONTEXTUAL ANALYSIS:
   - Background consistency
   - Lighting direction consistency
   - Reflection accuracy
   - Physics of movement (hair, clothing, objects)

Provide your analysis in a structured format with:
- Overall verdict (AI_GENERATED, AUTHENTIC, or UNCERTAIN)
- Confidence level (0-100%)
- Detailed reasoning
- Specific indicators found
- Recommendations for further investigation

Be thorough but acknowledge limitations when image quality or context is insufficient."""

    def __init__(self, model_provider: str = "ollama", model_name: str = "llava"):
        """Initialize the video fraud detection agent.

        Args:
            model_provider: The LLM provider ('ollama', 'openai', 'anthropic')
            model_name: The specific model to use
        """
        self.model_provider = model_provider
        self.model_name = model_name
        self._temp_dir: str | None = None

    def analyze_frame(self, frame_path: str | Path) -> AnalysisResult:
        """Analyze a single video frame for AI generation indicators.

        Args:
            frame_path: Path to the frame image file

        Returns:
            AnalysisResult with verdict and analysis details
        """
        frame_path = Path(frame_path)
        if not frame_path.exists():
            raise FileNotFoundError(f"Frame not found: {frame_path}")

        # Load and encode the image
        image_data = self._load_image(frame_path)

        # Send to LLM for analysis
        response = self._query_llm(image_data, frame_path.name)

        # Parse response into structured result
        return self._parse_response(response)

    def analyze_video(self, video_path: str | Path, sample_frames: int = 5) -> AnalysisResult:
        """Analyze a video file for AI generation indicators.

        Extracts frames from the video, analyzes each frame, and returns
        a single aggregated verdict for the entire video.

        Args:
            video_path: Path to the video file
            sample_frames: Number of frames to sample for analysis

        Returns:
            AnalysisResult with aggregated verdict and analysis
        """
        video_path = Path(video_path)
        if not video_path.exists():
            raise FileNotFoundError(f"Video not found: {video_path}")

        try:
            # Extract frames from video
            frames = self._extract_frames(video_path, sample_frames)

            # Analyze each frame
            frame_results = []
            for idx, frame in enumerate(frames):
                print(f"Analyzing frame {idx + 1}/{len(frames)}...")
                result = self.analyze_frame(frame)
                frame_results.append(result)

            # Aggregate results into single verdict
            return self._aggregate_results(frame_results)
        finally:
            # Always clean up temp files
            self._cleanup_temp_files()

    def _load_image(self, image_path: Path) -> str:
        """Load and base64 encode an image.

        Args:
            image_path: Path to image file

        Returns:
            Base64 encoded image string
        """
        with open(image_path, "rb") as f:
            return base64.b64encode(f.read()).decode("utf-8")

    def _query_llm(self, image_data: str, context: str) -> str:
        """Query the LLM with the image for analysis.

        Args:
            image_data: Base64 encoded image
            context: Additional context about the image

        Returns:
            LLM response text
        """
        if self.model_provider == "ollama":
            return self._query_ollama(image_data, context)
        elif self.model_provider == "openai":
            return self._query_openai(image_data, context)
        elif self.model_provider == "anthropic":
            return self._query_anthropic(image_data, context)
        else:
            raise ValueError(f"Unknown model provider: {self.model_provider}")

    def _query_ollama(self, image_data: str, context: str) -> str:
        """Query Ollama with vision model.

        Args:
            image_data: Base64 encoded image
            context: Additional context

        Returns:
            Model response
        """
        import requests

        ollama_host = os.getenv("OLLAMA_HOST", "http://localhost:11434")

        prompt = f"""Analyze this video frame ({context}) for signs of AI generation.

Provide your analysis in the following JSON format:
{{
    "verdict": "AI_GENERATED" | "AUTHENTIC" | "UNCERTAIN",
    "confidence": 0-100,
    "reasoning": "detailed explanation",
    "indicators": ["indicator1", "indicator2", ...],
    "recommendations": ["recommendation1", ...]
}}"""

        response = requests.post(
            f"{ollama_host}/api/generate",
            json={
                "model": self.model_name,
                "prompt": prompt,
                "system": self.SYSTEM_PROMPT,
                "images": [image_data],
                "stream": False,
            },
            timeout=120,
        )
        response.raise_for_status()
        return response.json().get("response", "")

    def _query_openai(self, image_data: str, context: str) -> str:
        """Query OpenAI with vision model.

        Args:
            image_data: Base64 encoded image
            context: Additional context

        Returns:
            Model response
        """
        # Placeholder for OpenAI implementation
        raise NotImplementedError("OpenAI provider not yet implemented")

    def _query_anthropic(self, image_data: str, context: str) -> str:
        """Query Anthropic with vision model.

        Args:
            image_data: Base64 encoded image
            context: Additional context

        Returns:
            Model response
        """
        # Placeholder for Anthropic implementation
        raise NotImplementedError("Anthropic provider not yet implemented")

    def _extract_frames(self, video_path: Path, num_frames: int) -> list[Path]:
        """Extract sample frames from a video.

        Extracts evenly distributed frames from the video for analysis.

        Args:
            video_path: Path to video file
            num_frames: Number of frames to extract

        Returns:
            List of paths to extracted frame images
        """
        cap = cv2.VideoCapture(str(video_path))
        if not cap.isOpened():
            raise ValueError(f"Could not open video: {video_path}")

        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        if total_frames == 0:
            cap.release()
            raise ValueError(f"Video has no frames: {video_path}")

        # Calculate frame indices to extract (evenly distributed)
        if num_frames >= total_frames:
            frame_indices = list(range(total_frames))
        else:
            step = total_frames / num_frames
            frame_indices = [int(i * step) for i in range(num_frames)]

        # Create temp directory for frames
        self._temp_dir = tempfile.mkdtemp(prefix="video_fraud_")
        extracted_paths = []

        for idx, frame_idx in enumerate(frame_indices):
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()

            if ret:
                frame_path = Path(self._temp_dir) / f"frame_{idx:04d}.jpg"
                cv2.imwrite(str(frame_path), frame)
                extracted_paths.append(frame_path)

        cap.release()

        if not extracted_paths:
            raise ValueError(f"Could not extract any frames from: {video_path}")

        return extracted_paths

    def _cleanup_temp_files(self) -> None:
        """Clean up temporary frame files.

        Removes all extracted frames and the temp directory.
        """
        import shutil

        if self._temp_dir and os.path.exists(self._temp_dir):
            shutil.rmtree(self._temp_dir)
            self._temp_dir = None

    def _parse_response(self, response: str) -> AnalysisResult:
        """Parse LLM response into structured result.

        Args:
            response: Raw LLM response text

        Returns:
            Structured AnalysisResult
        """
        try:
            # Try to extract JSON from response
            json_start = response.find("{")
            json_end = response.rfind("}") + 1
            if json_start >= 0 and json_end > json_start:
                json_str = response[json_start:json_end]
                data = json.loads(json_str)

                verdict_map = {
                    "AI_GENERATED": Verdict.AI_GENERATED,
                    "AUTHENTIC": Verdict.AUTHENTIC,
                    "UNCERTAIN": Verdict.UNCERTAIN,
                }

                return AnalysisResult(
                    verdict=verdict_map.get(data.get("verdict", "UNCERTAIN"), Verdict.UNCERTAIN),
                    confidence=float(data.get("confidence", 50)) / 100.0,
                    reasoning=data.get("reasoning", "No reasoning provided"),
                    indicators=data.get("indicators", []),
                    recommendations=data.get("recommendations", []),
                )
        except (json.JSONDecodeError, KeyError, ValueError):
            pass

        # Fallback for non-JSON responses
        return AnalysisResult(
            verdict=Verdict.UNCERTAIN,
            confidence=0.0,
            reasoning=response,
            indicators=[],
            recommendations=["Manual review recommended due to parsing issues"],
        )

    def _aggregate_results(self, results: list[AnalysisResult]) -> AnalysisResult:
        """Aggregate multiple frame results into overall verdict.

        Args:
            results: List of individual frame analysis results

        Returns:
            Aggregated AnalysisResult
        """
        if not results:
            return AnalysisResult(
                verdict=Verdict.UNCERTAIN,
                confidence=0.0,
                reasoning="No frames analyzed",
                indicators=[],
                recommendations=["Provide video frames for analysis"],
            )

        # Count verdicts
        verdict_counts = {v: 0 for v in Verdict}
        total_confidence = 0.0
        all_indicators = []
        all_recommendations = []

        for result in results:
            verdict_counts[result.verdict] += 1
            total_confidence += result.confidence
            all_indicators.extend(result.indicators)
            all_recommendations.extend(result.recommendations)

        # Determine overall verdict by majority
        max_verdict = max(verdict_counts, key=lambda v: verdict_counts[v])
        avg_confidence = total_confidence / len(results)

        # Deduplicate indicators and recommendations
        unique_indicators = list(set(all_indicators))
        unique_recommendations = list(set(all_recommendations))

        reasoning = (
            f"Analyzed {len(results)} frames. "
            f"Verdicts: AI={verdict_counts[Verdict.AI_GENERATED]}, "
            f"Authentic={verdict_counts[Verdict.AUTHENTIC]}, "
            f"Uncertain={verdict_counts[Verdict.UNCERTAIN]}. "
            f"Average confidence: {avg_confidence:.1%}"
        )

        return AnalysisResult(
            verdict=max_verdict,
            confidence=avg_confidence,
            reasoning=reasoning,
            indicators=unique_indicators,
            recommendations=unique_recommendations,
        )
