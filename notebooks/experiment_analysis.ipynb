{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# AI Video Fraud Detection - Experiment Analysis\n",
    "\n",
    "This notebook provides detailed statistical analysis of the video fraud detection experiment results.\n",
    "\n",
    "## Contents\n",
    "1. Data Loading and Overview\n",
    "2. Performance Metrics Analysis\n",
    "3. Confidence Score Analysis\n",
    "4. Error Analysis\n",
    "5. Visualizations\n",
    "6. Statistical Tests\n",
    "7. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Load metrics data\n",
    "metrics_path = Path('../results/metrics/experiment_metrics.json')\n",
    "with open(metrics_path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Experiment: {data['experiment']['name']}\")\n",
    "print(f\"Date: {data['experiment']['date']}\")\n",
    "print(f\"Dataset size: {data['experiment']['dataset_size']} videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-overview",
   "metadata": {},
   "source": [
    "## 1. Data Overview\n",
    "\n",
    "### Dataset Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset composition\n",
    "dataset = data['dataset']\n",
    "print(f\"Total videos: {dataset['total_videos']}\")\n",
    "print(f\"AI-generated: {dataset['ai_generated']}\")\n",
    "print(f\"Authentic: {dataset['authentic']}\")\n",
    "print(f\"\\nClass distribution: {dataset['ai_generated']/dataset['total_videos']*100:.1f}% AI / {dataset['authentic']/dataset['total_videos']*100:.1f}% Authentic\")\n",
    "\n",
    "# Dataset table\n",
    "print(\"\\n--- Dataset Summary ---\")\n",
    "for video in dataset['videos']:\n",
    "    print(f\"Video {video['id']}: {video['file']:15s} -> {video['ground_truth']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics-section",
   "metadata": {},
   "source": [
    "## 2. Performance Metrics\n",
    "\n",
    "### Classification Metrics\n",
    "\n",
    "The standard binary classification metrics are:\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "$$F_1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metrics-calc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract confusion matrix values\n",
    "cm = data['confusion_matrix']\n",
    "tp = cm['true_positives']\n",
    "tn = cm['true_negatives']\n",
    "fp = cm['false_positives']\n",
    "fn = cm['false_negatives']\n",
    "\n",
    "# Calculate metrics (verify against stored values)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"=\" * 40)\n",
    "print(\"PERFORMANCE METRICS\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Accuracy:  {accuracy:.1%} ({tp + tn}/{tp + tn + fp + fn})\")\n",
    "print(f\"Precision: {precision:.1%} ({tp}/{tp + fp})\")\n",
    "print(f\"Recall:    {recall:.1%} ({tp}/{tp + fn})\")\n",
    "print(f\"F1 Score:  {f1:.3f}\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Visualization\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "cm_array = np.array([[tp, fn], [fp, tn]])\n",
    "labels = ['AI Generated', 'Authentic']\n",
    "\n",
    "sns.heatmap(cm_array, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Predicted AI', 'Predicted Authentic'],\n",
    "            yticklabels=['Actual AI', 'Actual Authentic'],\n",
    "            ax=ax, cbar_kws={'label': 'Count'})\n",
    "\n",
    "ax.set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax.set_ylabel('Actual Label', fontsize=12)\n",
    "\n",
    "# Add annotations\n",
    "ax.text(0, -0.15, f'TP={tp}', transform=ax.transAxes, fontsize=10)\n",
    "ax.text(0.25, -0.15, f'FN={fn}', transform=ax.transAxes, fontsize=10)\n",
    "ax.text(0.5, -0.15, f'FP={fp}', transform=ax.transAxes, fontsize=10)\n",
    "ax.text(0.75, -0.15, f'TN={tn}', transform=ax.transAxes, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidence-section",
   "metadata": {},
   "source": [
    "## 3. Confidence Score Analysis\n",
    "\n",
    "Analyzing the relationship between model confidence and prediction correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidence-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract predictions\n",
    "predictions = data['predictions']\n",
    "\n",
    "# Separate correct and incorrect predictions\n",
    "correct_conf = [p['confidence'] for p in predictions if p['correct']]\n",
    "incorrect_conf = [p['confidence'] for p in predictions if not p['correct']]\n",
    "\n",
    "print(\"=\" * 40)\n",
    "print(\"CONFIDENCE ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Correct predictions ({len(correct_conf)}):\")\n",
    "print(f\"  Mean confidence: {np.mean(correct_conf):.1f}%\")\n",
    "print(f\"  Std deviation:   {np.std(correct_conf):.1f}%\")\n",
    "print(f\"  Range: {min(correct_conf)}% - {max(correct_conf)}%\")\n",
    "print()\n",
    "print(f\"Incorrect predictions ({len(incorrect_conf)}):\")\n",
    "print(f\"  Mean confidence: {np.mean(incorrect_conf):.1f}%\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Key finding\n",
    "if np.mean(incorrect_conf) >= np.mean(correct_conf):\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Incorrect predictions have equal or higher confidence!\")\n",
    "    print(\"   This indicates confidence scores are not well-calibrated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidence-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Per-video confidence\n",
    "ax1 = axes[0]\n",
    "video_ids = [p['video_id'] for p in predictions]\n",
    "confidences = [p['confidence'] for p in predictions]\n",
    "colors = ['green' if p['correct'] else 'red' for p in predictions]\n",
    "\n",
    "bars = ax1.bar(video_ids, confidences, color=colors, edgecolor='black', alpha=0.8)\n",
    "ax1.axhline(y=np.mean(confidences), color='blue', linestyle='--', label=f'Mean: {np.mean(confidences):.1f}%')\n",
    "ax1.set_xlabel('Video ID', fontsize=12)\n",
    "ax1.set_ylabel('Confidence (%)', fontsize=12)\n",
    "ax1.set_title('Confidence by Video', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim(0, 105)\n",
    "ax1.legend()\n",
    "\n",
    "# Add correct/incorrect labels\n",
    "for i, (vid, conf, pred) in enumerate(zip(video_ids, confidences, predictions)):\n",
    "    label = '‚úì' if pred['correct'] else '‚úó'\n",
    "    ax1.text(vid, conf + 2, label, ha='center', fontsize=14)\n",
    "\n",
    "# Confidence distribution by correctness\n",
    "ax2 = axes[1]\n",
    "data_to_plot = [correct_conf, incorrect_conf] if incorrect_conf else [correct_conf]\n",
    "labels_plot = ['Correct', 'Incorrect'] if incorrect_conf else ['Correct']\n",
    "\n",
    "bp = ax2.boxplot(data_to_plot, labels=labels_plot, patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('lightgreen')\n",
    "if len(bp['boxes']) > 1:\n",
    "    bp['boxes'][1].set_facecolor('lightcoral')\n",
    "\n",
    "ax2.set_ylabel('Confidence (%)', fontsize=12)\n",
    "ax2.set_title('Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/confidence_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "error-section",
   "metadata": {},
   "source": [
    "## 4. Error Analysis\n",
    "\n",
    "Detailed examination of classification errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "error-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis\n",
    "error_info = data['error_analysis']\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total errors: {error_info['total_errors']}\")\n",
    "print(f\"Error type: {error_info['error_type']}\")\n",
    "print(f\"Error video: Video {error_info['error_video']}\")\n",
    "print(f\"\\nDescription:\")\n",
    "print(f\"  {error_info['error_description']}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Find the error prediction details\n",
    "error_pred = next(p for p in predictions if not p['correct'])\n",
    "print(f\"\\nError Details:\")\n",
    "print(f\"  Video ID: {error_pred['video_id']}\")\n",
    "print(f\"  Prediction: {error_pred['prediction']}\")\n",
    "print(f\"  Confidence: {error_pred['confidence']}%\")\n",
    "print(f\"  Ground Truth: AUTHENTIC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precision-recall",
   "metadata": {},
   "source": [
    "## 5. Precision-Recall Trade-off\n",
    "\n",
    "Understanding the trade-off between precision and recall in our detection system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pr-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics comparison visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "metrics_values = [accuracy, precision, recall, f1]\n",
    "colors = ['steelblue', 'coral', 'seagreen', 'mediumpurple']\n",
    "\n",
    "bars = ax.bar(metrics_names, metrics_values, color=colors, edgecolor='black', alpha=0.8)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, metrics_values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "            f'{val:.1%}' if val <= 1 else f'{val:.3f}',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.axhline(y=0.75, color='red', linestyle='--', alpha=0.7, label='75% threshold')\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Classification Performance Metrics', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 1.15)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/metrics_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-tests",
   "metadata": {},
   "source": [
    "## 6. Statistical Considerations\n",
    "\n",
    "### Sample Size Limitations\n",
    "\n",
    "With only $n=5$ samples, we must be cautious about statistical inference:\n",
    "\n",
    "$$\\text{Standard Error} = \\sqrt{\\frac{p(1-p)}{n}}$$\n",
    "\n",
    "For our accuracy of 80%:\n",
    "$$SE = \\sqrt{\\frac{0.8 \\times 0.2}{5}} \\approx 0.179$$\n",
    "\n",
    "This gives a 95% confidence interval of approximately:\n",
    "$$0.80 \\pm 1.96 \\times 0.179 = [0.45, 1.00]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-calc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical calculations\n",
    "n = 5\n",
    "p = accuracy\n",
    "\n",
    "# Standard error\n",
    "se = np.sqrt(p * (1 - p) / n)\n",
    "\n",
    "# 95% CI (normal approximation - note: not ideal for small n)\n",
    "z = 1.96\n",
    "ci_lower = max(0, p - z * se)\n",
    "ci_upper = min(1, p + z * se)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"STATISTICAL ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Sample size: n = {n}\")\n",
    "print(f\"Observed accuracy: p = {p:.1%}\")\n",
    "print(f\"Standard error: SE = {se:.3f}\")\n",
    "print(f\"95% Confidence Interval: [{ci_lower:.1%}, {ci_upper:.1%}]\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n‚ö†Ô∏è NOTE: Small sample size limits statistical power.\")\n",
    "print(\"   Results should be validated with larger datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusions",
   "metadata": {},
   "source": [
    "## 7. Conclusions\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Accuracy**: 80% overall accuracy, meeting the >75% target\n",
    "2. **High Recall**: 100% recall indicates all AI-generated content was detected\n",
    "3. **False Positive Risk**: One authentic video was incorrectly flagged as AI\n",
    "4. **Confidence Calibration**: High confidence does not guarantee correctness\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "1. Expand dataset for more robust statistical conclusions\n",
    "2. Implement confidence calibration techniques\n",
    "3. Add compression artifact awareness to the prompt\n",
    "4. Consider ensemble methods combining multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìä Dataset: {n} videos ({dataset['ai_generated']} AI, {dataset['authentic']} authentic)\")\n",
    "print(f\"\\nüìà Results:\")\n",
    "print(f\"   ‚Ä¢ Accuracy:  {accuracy:.1%}\")\n",
    "print(f\"   ‚Ä¢ Precision: {precision:.1%}\")\n",
    "print(f\"   ‚Ä¢ Recall:    {recall:.1%}\")\n",
    "print(f\"   ‚Ä¢ F1 Score:  {f1:.3f}\")\n",
    "print(f\"\\n‚ö†Ô∏è Errors: {error_info['total_errors']} ({error_info['error_type']})\")\n",
    "print(f\"\\n‚úÖ Conclusion: Model shows promise but needs larger validation\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
